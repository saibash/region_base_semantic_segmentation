{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Region-Based Edge Convolutions With Geometric Attributes for the Semantic Segmentation of Large-Scale 3-D Point Clouds \n",
    "\n",
    "This is an implementation of the paper:\n",
    "https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9103287\n",
    "\n",
    "by : Jhonatan Contreras \n",
    "\n",
    "## Code structure\n",
    "\n",
    "###  ./segmentation_and_rebuild \n",
    "  - ./partition/segmentation.py - geometric partitioning and segments construction using handcrafted features.\n",
    "  - ./partition/write_segments_to_pc.py - upsample the prediction to the original point clouds..\n",
    "  \n",
    "###  ./semantic_segmentation\n",
    " - ./gpu_main.py -  supervised learning code semantic segmentation and inference.\n",
    " \n",
    "\n",
    "## Disclaimer\n",
    "\n",
    "The partition method is stochastic. The results obtained could differ slightly. The original partition work was developed for Loic Landrieu, and presented in the work: \n",
    "\n",
    "<i>A structured regularization framework for spatially smoothing semantic labelings of 3D point clouds. Loic Landrieu, Hugo Raguet , Bruno Vallet , Cl√©ment Mallet, Martin Weinmann</i>\n",
    "\n",
    "\n",
    "## Requirements\n",
    "    \n",
    "1. Download current version of the repository.\n",
    "2. Follow the document: <b> installation.ipynb </b> \n",
    "    \n",
    "\n",
    "## Datasets and visualization\n",
    "\n",
    "In order to use our code, the dataset must be divided into diferent subfolders, e.g. <b>testing/, training/, validation/</b>.\n",
    "\n",
    "The input files \".txt\" and \".npy\" are supported. It is recommended that other formats are first transformed to \".txt\" using a third party software, e.g. CloudCompare &copy;\n",
    "\n",
    "CloudCompare &copy; could be used to visualize intermediate and final results.\n",
    "To install CloudCompare &copy; use the command line:\n",
    "\n",
    "    $ sudo snap install cloudcompare\n",
    "\n",
    "\n",
    "\n",
    "## Running the code\n",
    "\n",
    "To run our code from scratch on different datasets, we need to complete four stages:\n",
    "\n",
    " <ol>\n",
    "  <li>Segmentation</li>\n",
    "  <li>Training</li>\n",
    "  <li>Inference</li>\n",
    "  <li>Upsampling</li>\n",
    "</ol>  \n",
    "\n",
    "\n",
    "   \n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Segmentation \n",
    "\n",
    "We recomend to follow the file <b> ./segmentation_and_rebuild/main.sh </b> as guide using the sample dataset <b><i>forest4D_dlr</t></b>.\n",
    "\n",
    "The code <b>./partition/segmentation.py </b> has the following arguments:\n",
    "    \n",
    "#### Required arguments.\n",
    "\n",
    " <table style=\"width:100%\">\n",
    "  <tr>\n",
    "    <th>Argument</th>\n",
    "    <th>default</th>\n",
    "    <th>type</th> \n",
    "    <th>help</th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>--path_to_data</td>\n",
    "    <td>\"../Data/forest4D_dlr\"</td>\n",
    "    <td>str</td>\n",
    "    <td>Path to data</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>--path_to_output</td>\n",
    "    <td>\"../Data/forest4D_dlr\"</td>\n",
    "    <td>str</td>\n",
    "    <td>Path to output</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>--n_labels</td>\n",
    "    <td>4</td>\n",
    "    <td>int</td>\n",
    "    <td>number of classes</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>--areas</td>\n",
    "    <td>\"testing/, training/, validation/\"</td>\n",
    "    <td>str</td>\n",
    "    <td>\"list of subfolders to be processed separated by ( , )</td>\n",
    "  </tr> \n",
    "</table> \n",
    "\n",
    "\n",
    "#### Optional arguments.\n",
    "  <table style=\"width:100%\">\n",
    "  <tr>\n",
    "    <th>Argument</th>\n",
    "    <th>default</th>\n",
    "    <th>type</th> \n",
    "    <th>help</th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>--RGB</td>\n",
    "    <td>\"False\"</td>\n",
    "    <td>bool</td>\n",
    "    <td>True if Data set contains RGB information</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>--version</td>\n",
    "    <td>\"V0\"</td>\n",
    "    <td>str</td>\n",
    "    <td>for multiples segmentation parameters, use a different output name </td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>--file_extension</td>\n",
    "    <td>\".txt\"</td>\n",
    "    <td>str</td>\n",
    "    <td>file extension</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>--gt_index</td>\n",
    "    <td>3</td>\n",
    "    <td>int</td>\n",
    "    <td>ground true index in file</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>--rgb_intensity_index</td>\n",
    "    <td>[3,4,5]</td>\n",
    "    <td>list</td>\n",
    "    <td>rgb or intensity index in file</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>--ver_batch</td>\n",
    "    <td>2000000</td>\n",
    "    <td>int</td>\n",
    "    <td>Batch size for reading large files</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>--voxel_width</td>\n",
    "    <td>.01</td>\n",
    "    <td>float</td>\n",
    "    <td>voxel size when subsampling (in m)</td>\n",
    "  </tr>\n",
    "    <tr>\n",
    "    <td>--k_nn_geof</td>\n",
    "    <td>45</td>\n",
    "    <td>int</td>\n",
    "    <td>number of neighbors for the geometric features</td>\n",
    "  </tr>\n",
    "    <tr>\n",
    "    <td>--k_nn_adj</td>\n",
    "    <td>10</td>\n",
    "    <td>int</td>\n",
    "    <td>adjacency structure for the minimal partition</td>\n",
    "  </tr>\n",
    "    <tr>\n",
    "    <td>--lambda_edge_weight</td>\n",
    "    <td>1.</td>\n",
    "    <td>float</td>\n",
    "    <td>parameter determine the edge weight for minimal part.</td>\n",
    "  </tr>\n",
    "   <tr>\n",
    "    <td>--reg_strength</td>\n",
    "    <td>1.</td>\n",
    "    <td>float</td>\n",
    "    <td>regularization strength for the minimal partition</td>\n",
    "  </tr>\n",
    "   <tr>\n",
    "    <td>--sorted</td>\n",
    "    <td>False</td>\n",
    "    <td>bool</td>\n",
    "    <td>Reverse order to read the data</td>\n",
    "  </tr>\n",
    "   <tr>\n",
    "    <td>--overwrite</td>\n",
    "    <td>False</td>\n",
    "    <td>bool</td>\n",
    "    <td>Consider previous results</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>--print_progress</td>\n",
    "    <td>True</td>\n",
    "    <td>bool</td>\n",
    "    <td>Print current process</td>\n",
    "  </tr>\n",
    "</table> \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upsampling \n",
    "\n",
    "We recomend to follow the file <b> ./segmentation_and_rebuild/main.sh </b> as guide using the sample dataset <b><i>forest4D_dlr</t></b>.\n",
    "\n",
    "The code <b>./partition/write_segments_to_pc.py </b> has the following arguments:\n",
    "    \n",
    "#### Required arguments.\n",
    "\n",
    " <table style=\"width:100%\">\n",
    "  <tr>\n",
    "    <th>Argument</th>\n",
    "    <th>default</th>\n",
    "    <th>type</th> \n",
    "    <th>help</th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>--path_to_data</td>\n",
    "    <td>\"../Data/forest4D_dlr\"</td>\n",
    "    <td>str</td>\n",
    "    <td>Path to data</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>--path_to_output</td>\n",
    "    <td>\"../Data/forest4D_dlr\"</td>\n",
    "    <td>str</td>\n",
    "    <td>Directory to store results</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>--areas</td>\n",
    "    <td>\"testing/, training/, validation/\"</td>\n",
    "    <td>str</td>\n",
    "    <td>areas to be processed</td>\n",
    "  </tr>\n",
    "      <tr>\n",
    "    <td>--n_classes</td>\n",
    "    <td>4</td>\n",
    "    <td>int</td>\n",
    "    <td>number of classes</td>\n",
    "  </tr>\n",
    "      <tr>\n",
    "    <td>--metrics</td>\n",
    "    <td>False</td>\n",
    "    <td>bool</td>\n",
    "    <td>Compute metrics</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>--file_extension</td>\n",
    "    <td>\".txt\"</td>\n",
    "    <td>str</td>\n",
    "    <td>file extension default txt</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>--gt_index</td>\n",
    "    <td>3</td>\n",
    "    <td>int</td>\n",
    "    <td>ground true index in file</td>\n",
    "  </tr>\n",
    "      <tr>\n",
    "    <td>--ver_batch</td>\n",
    "    <td>500000</td>\n",
    "    <td>int</td>\n",
    "    <td>Batch size for reading large files</td>\n",
    "  </tr>\n",
    "</table>   \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and Inference\n",
    "\n",
    "We recomend to follow the file <b> ./semantic_segmentation/main.sh </b> as guide using the sample dataset <b><i>forest4D_dlr</t></b>.\n",
    "\n",
    "The code <b>./semantic_segmentation/gpu_main.py </b> has the following arguments:\n",
    "\n",
    "\n",
    "- ### Computation configuration\n",
    "#### Required arguments \n",
    "\n",
    "<table style=\"width:100%\">\n",
    "  <tr>\n",
    "    <th>Argument</th>\n",
    "    <th>default</th>\n",
    "    <th>type</th> \n",
    "    <th>help</th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>--training</td>\n",
    "    <td>True</td>\n",
    "    <td>bool</td>\n",
    "    <td>True:Training, False:Testing </td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>--only_test</td>\n",
    "    <td>True</td>\n",
    "    <td>bool</td>\n",
    "    <td>True:Evaluate test, False:evaluate training and testing </td>\n",
    "  </tr> \n",
    "  <tr>\n",
    "    <td>--batch_size</td>\n",
    "    <td>1024</td>\n",
    "    <td>int</td>\n",
    "    <td>Batch size for training </td>\n",
    "  </tr>  \n",
    "  <tr>\n",
    "    <td>--n_epochs</td>\n",
    "    <td>1000</td>\n",
    "    <td>int</td>\n",
    "    <td>Number of epochs </td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>--freq_aug</td>\n",
    "    <td>5</td>\n",
    "    <td>int</td>\n",
    "    <td>Frequency in epochs of training augmentation </td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>--freq_validation</td>\n",
    "    <td>5</td>\n",
    "    <td>int</td>\n",
    "    <td>Frequency validation in epochs </td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>--num_gpus</td>\n",
    "    <td>1</td>\n",
    "    <td>int</td>\n",
    "    <td>How many GPUs to use </td>\n",
    "  </tr>  \n",
    "  <tr>\n",
    "    <td>--gpu_memory_fraction</td>\n",
    "    <td>.5</td>\n",
    "    <td>int</td>\n",
    "    <td>Fraction of the GPU to be allocated to that process, -1 to choose automatically </td>\n",
    "  </tr>\n",
    "</table> \n",
    "\n",
    "- ### Dataset    \n",
    "#### Required arguments \n",
    "\n",
    "<table style=\"width:100%\">\n",
    "  <tr>\n",
    "    <th>Argument</th>\n",
    "    <th>default</th>\n",
    "    <th>type</th> \n",
    "    <th>help</th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>--odir</td>\n",
    "    <td>\"log\"</td>\n",
    "    <td>str</td>\n",
    "    <td>Output directory to store logs</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>--log_file</td>\n",
    "    <td>\"log_doc\"</td>\n",
    "    <td>str</td>\n",
    "    <td>Log file to store results</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>--db_train_name</td>\n",
    "    <td>\"training\"</td>\n",
    "    <td>str</td>\n",
    "    <td>Training folder</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>--db_test_name</td>\n",
    "    <td>\"testing\"</td>\n",
    "    <td>str</td>\n",
    "    <td>Testing or validation folder</td>\n",
    "  </tr>\n",
    "    <tr>\n",
    "    <td>--SEMA3D_PATH</td>\n",
    "    <td>'../Data/forest4D_dlr'</td>\n",
    "    <td>str</td>\n",
    "    <td>Dataset directory</td>\n",
    "  </tr>\n",
    "    <tr>\n",
    "    <td>--model_name</td>\n",
    "    <td>'dlr_new_model'</td>\n",
    "    <td>str</td>\n",
    "    <td>model_name</td>\n",
    "  </tr>\n",
    "    \n",
    "</table>   \n",
    "\n",
    "   \n",
    "#### Optional arguments.\n",
    "\n",
    "<table style=\"width:100%\">\n",
    "  <tr>\n",
    "    <th>Argument</th>\n",
    "    <th>default</th>\n",
    "    <th>type</th> \n",
    "    <th>help</th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>--dataset</td>\n",
    "    <td>'other'</td>\n",
    "    <td>str</td>\n",
    "    <td>'Dataset name: sema3d|s3dis'</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>--resume</td>\n",
    "    <td>True</td>\n",
    "    <td>bool</td>\n",
    "    <td>True: load a previously saved model</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>--resume_best_val</td>\n",
    "    <td>False</td>\n",
    "    <td>bool</td>\n",
    "    <td>True: load model from best validation result</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>--restoring_partially</td>\n",
    "    <td>False</td>\n",
    "    <td>bool</td>\n",
    "    <td>True: to initiallize with pretraining model</td>\n",
    "  </tr>\n",
    "   <tr>\n",
    "    <td>--pre_train</td>\n",
    "    <td>'dlr_old_model'</td>\n",
    "    <td>str</td>\n",
    "    <td>\"pretrain model to initialize new model\"</td>\n",
    "  </tr>\n",
    "</table>   \n",
    "\n",
    "\n",
    "- ### Point cloud pre-processing\n",
    "#### Optional arguments \n",
    "\n",
    "<table style=\"width:100%\">\n",
    "  <tr>\n",
    "    <th>Argument</th>\n",
    "    <th>default</th>\n",
    "    <th>type</th> \n",
    "    <th>help</th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>--pc_xyznormalize</td>\n",
    "    <td>True</td>\n",
    "    <td>bool</td>\n",
    "    <td>Bool, normalize xyz into unit ball, i.e. in [-0.5,0.5]</td>\n",
    "  </tr>\n",
    "   <tr>\n",
    "    <td>--pc_augm</td>\n",
    "    <td>False</td>\n",
    "    <td>bool</td>\n",
    "    <td>Training augmentation</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>--pc_augm_scale</td>\n",
    "    <td>1.1</td>\n",
    "    <td>float</td>\n",
    "    <td>Training augmentation: Uniformly random scaling in [1/scale, scale] </td>\n",
    "  </tr>\n",
    "   <tr>\n",
    "    <td>--pc_augm_rot</td>\n",
    "    <td>True</td>\n",
    "    <td>bool</td>\n",
    "    <td>Training augmentation: Bool, random rotation around z-axis </td>\n",
    "  </tr>\n",
    "   <tr>\n",
    "    <td>--pc_augm_mirror_prob</td>\n",
    "    <td>False</td>\n",
    "    <td>bool</td>\n",
    "    <td>Training augmentation: Probability of mirroring about x or y axes</td>\n",
    "  </tr>\n",
    "     <tr>\n",
    "    <td>--pc_augm_jitter</td>\n",
    "    <td>False</td>\n",
    "    <td>bool</td>\n",
    "    <td>Training augmentation: Bool, Gaussian jittering of all attributes </td>\n",
    "  </tr>\n",
    "     <tr>\n",
    "    <td>--spg_attribs01</td>\n",
    "    <td>True</td>\n",
    "    <td>bool</td>\n",
    "    <td>Bool, normalize edge features to 0 mean 1 deviation </td>\n",
    "  </tr>    \n",
    "</table>   \n",
    "\n",
    "- ### Model configuration\n",
    "#### Required arguments \n",
    "\n",
    "<table style=\"width:100%\">\n",
    "  <tr>\n",
    "    <th>Argument</th>\n",
    "    <th>default</th>\n",
    "    <th>type</th> \n",
    "    <th>help</th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>--n_classes</td>\n",
    "    <td>3</td>\n",
    "    <td>int</td>\n",
    "    <td>Number of classes</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>--pc_xyznormalize</td>\n",
    "    <td>True</td>\n",
    "    <td>bool</td>\n",
    "    <td>Bool</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>--pc_xyznormalize</td>\n",
    "    <td>True</td>\n",
    "    <td>bool</td>\n",
    "    <td>Bool</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>--pc_xyznormalize</td>\n",
    "    <td>True</td>\n",
    "    <td>bool</td>\n",
    "    <td>Bool</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>--pc_xyznormalize</td>\n",
    "    <td>True</td>\n",
    "    <td>bool</td>\n",
    "    <td>Bool</td>\n",
    "  </tr>\n",
    "\n",
    "\n",
    "\n",
    "</table>   \n",
    "\n",
    "#### Optional arguments \n",
    "\n",
    "<table style=\"width:100%\">\n",
    "  <tr>\n",
    "    <th>Argument</th>\n",
    "    <th>default</th>\n",
    "    <th>type</th> \n",
    "    <th>help</th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>--ptn_minpts</td>\n",
    "    <td>40</td>\n",
    "    <td>int</td>\n",
    "    <td>Minimum number of points into a segment for computing its embedding</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>--ptn_npts</td>\n",
    "    <td>512</td>\n",
    "    <td>bool</td>\n",
    "    <td>Maximum number of points into a segment for computing its embedding</td>\n",
    "  </tr>\n",
    "    <tr>\n",
    "    <td>--rgb</td>\n",
    "    <td>False</td>\n",
    "    <td>bool</td>\n",
    "    <td>Consider RGB on Training</td>\n",
    "  </tr>\n",
    "      <tr>\n",
    "    <td>--learning_rate</td>\n",
    "    <td>0.01</td>\n",
    "    <td>float</td>\n",
    "    <td>initial learning rate for training</td>\n",
    "  </tr>\n",
    " <tr>\n",
    "    <td>--class_weights</td>\n",
    "    <td>False</td>\n",
    "    <td>bool</td>\n",
    "    <td>Compute class weights for imbalanced datasets</td>\n",
    "  </tr>\n",
    "     <tr>\n",
    "    <td>--sor</td>\n",
    "    <td>False</td>\n",
    "    <td>bool</td>\n",
    "    <td>Statistical Outlier Removal</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>--mean_k</td>\n",
    "    <td>30</td>\n",
    "    <td>float</td>\n",
    "    <td>mean_k for Statistical Outlier Removal</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>--std_dev</td>\n",
    "    <td>1.0</td>\n",
    "    <td>float</td>\n",
    "    <td>std_dev for Statistical Outlier Removal</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "  <td>--l2_norm</td>\n",
    "    <td>False</td>\n",
    "    <td>bool</td>\n",
    "    <td>l2 normalization</td>\n",
    "  </tr>    \n",
    "</table> \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
